{"name":"Machine learning","tagline":"Machine Learning using Random Forests","body":"\r\n<div class=\"container-fluid main-container\">\r\n\r\n<div id=\"header\">\r\n<h1 class=\"title\">Machine Learning Prediction Writeup</h1>\r\n</div>\r\n\r\n<div id=\"human-activity-recognition-with-wearable-accelerometers\" class=\"section level2\">\r\n<h2>Human Activity Recognition with Wearable Accelerometers</h2>\r\n<div id=\"data-description\" class=\"section level3\">\r\n<h3>Data Description</h3>\r\n<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit, it is now easy to collect a large amount of data about personal activity. These types of devices are part of the “quantified self movement - a group of enthusiasts who take measurements of their actions regularly. Although some people often quantify how much of a particular activity they do, they rarely quantify how well they do it. This project uses data from accelerometers on the belt, forearm, arm, and dumbell of six participants. They were asked to perform barbell lifts correctly and incorrectly in five different ways:</p>\r\n<pre><code>1. Exactly according to the specification\r\n2. Throwing elbows to the front\r\n3. Lifting the dumbbell only halfway\r\n4. lowering the dumbbell only halfway\r\n5. throwing the hips to the front</code></pre>\r\n<p>More information is available from the website here: <a href=\"http://groupware.les.inf.puc-rio.br/har\" class=\"uri\">http://groupware.les.inf.puc-rio.br/har</a> (see the section on the Weight Lifting Exercise Dataset).</p>\r\n</div>\r\n<div id=\"objectives\" class=\"section level3\">\r\n<h3>Objectives</h3>\r\n<p>The goal of this project was to predict the manner (see above) in which the subjects did the exercise when given relevant information.</p>\r\n</div>\r\n<div id=\"preparing-the-data\" class=\"section level3\">\r\n<h3>Preparing the Data</h3>\r\n<p>The data was obtained from: <a href=\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\" class=\"uri\">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>\r\n<p>The data source has 159 variables. The first seven columns of the data (<i>X, user name, raw timestamp part 1, raw timestamp part 2, cvtd timestamp, new window, num window</i>) will be removed, since these do not appear to be accelerometer measurements which will not have an effect in prediction.</p>\r\n<p>Sparse variables have few observations, so they have weak predictive value. These variables will be removed unless at least 80% of their observations are present.</p>\r\n<p>Variables with few unique values are also removed, since their invariance adds little to the predictive accuracy of the model.</p>\r\n<p>Lastly, variables that are too highly correlated are removed since they are not very useful. This step reduced the number of variables in the data from 53 to 46.</p>\r\n<pre class=\"r\"><code>dim(data.training)</code></pre>\r\n<pre><code>## [1] 19622   159</code></pre>\r\n<pre class=\"r\"><code>data.training &lt;- data.training[,c(8:ncol(data.training))] #from 159 vars\r\n\r\ndim(data.training)</code></pre>\r\n<pre><code>## [1] 19622   152</code></pre>\r\n<pre class=\"r\"><code>data.training &lt;- data.training[,colSums(is.na(data.training)) &lt; .8]\r\n\r\ndim(data.training)</code></pre>\r\n<pre><code>## [1] 19622   119</code></pre>\r\n<pre class=\"r\"><code>nsv &lt;- nearZeroVar(data.training, saveMetrics = TRUE)\r\ndata.training &lt;- data.training[,!nsv$nzv]\r\n\r\ndim(data.training)</code></pre>\r\n<pre><code>## [1] 19622    52</code></pre>\r\n<pre class=\"r\"><code>highCorrelations &lt;- cor(na.omit(data.training[sapply(data.training, is.numeric)]))\r\nhighCorr&lt;-findCorrelation(highCorrelations, cutoff = .90, verbose = FALSE)\r\ndata.training&lt;- data.training[,-highCorr]\r\n\r\ndim(data.training)</code></pre>\r\n<pre><code>## [1] 19622    46</code></pre>\r\n<p>This leaves us with 45 predictors and the “classe” variable in the training data.</p>\r\n<p>We will split the training data into a “training” set and a “validating” set. The training subset will include 75% of the observations and the remaining validation data will be used to calculate the out-of-sample error.</p>\r\n<pre class=\"r\"><code>xdata &lt;- createDataPartition(y = data.training$classe, p = .75, list = FALSE )\r\ndata.validating &lt;- data.training[-xdata,]\r\ndata.training &lt;- data.training[xdata,]</code></pre>\r\n<div id=\"random-forest-model\" class=\"section level4\">\r\n<h4>Random Forest Model</h4>\r\n<p>There does not seem to be any predictors strongly correlated with the outcome variable, so linear regression model may not be a good option. Instead, a classifcation algorithm – Random Forest model – will be used. This model uses bagging and random subsets of variables from the training data to prevent overfitting. In this algorithm, many different trees are created for cross validation.</p>\r\n<pre class=\"r\"><code>model.rf &lt;- randomForest(classe~., data = data.training)</code></pre>\r\n</div>\r\n<div id=\"training-statistics\" class=\"section level4\">\r\n<h4>Training Statistics</h4>\r\n<pre class=\"r\"><code>predict.rf &lt;- predict(model.rf, data.training, type = &quot;class&quot;)\r\nconfusionMatrix(predict.rf, data.training$classe)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 4185    0    0    0    0\r\n##          B    0 2848    0    0    0\r\n##          C    0    0 2567    0    0\r\n##          D    0    0    0 2412    0\r\n##          E    0    0    0    0 2706\r\n## \r\n## Overall Statistics\r\n##                                      \r\n##                Accuracy : 1          \r\n##                  95% CI : (0.9997, 1)\r\n##     No Information Rate : 0.2843     \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  \r\n##                                      \r\n##                   Kappa : 1          \r\n##  Mcnemar's Test P-Value : NA         \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Specificity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Pos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Neg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Prevalence             0.2843   0.1935   0.1744   0.1639   0.1839\r\n## Detection Rate         0.2843   0.1935   0.1744   0.1639   0.1839\r\n## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1839\r\n## Balanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000</code></pre>\r\n<p>Now we test our model on the out-of-sample dataset. The error is expected to be higher than with our out-of-sample data, but hopefully as close as possible.</p>\r\n</div>\r\n<div id=\"validation-statistics\" class=\"section level4\">\r\n<h4>Validation Statistics</h4>\r\n<pre class=\"r\"><code>predictions.validating &lt;- predict(model.rf, newdata = data.validating)\r\nconfusionMatrix(predictions.validating, data.validating$classe)</code></pre>\r\n<pre><code>## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1394    4    0    0    0\r\n##          B    0  938    7    0    0\r\n##          C    0    7  847    7    1\r\n##          D    0    0    1  796    0\r\n##          E    1    0    0    1  900\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.9941         \r\n##                  95% CI : (0.9915, 0.996)\r\n##     No Information Rate : 0.2845         \r\n##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.9925         \r\n##  Mcnemar's Test P-Value : NA             \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9993   0.9884   0.9906   0.9900   0.9989\r\n## Specificity            0.9989   0.9982   0.9963   0.9998   0.9995\r\n## Pos Pred Value         0.9971   0.9926   0.9826   0.9987   0.9978\r\n## Neg Pred Value         0.9997   0.9972   0.9980   0.9981   0.9998\r\n## Prevalence             0.2845   0.1935   0.1743   0.1639   0.1837\r\n## Detection Rate         0.2843   0.1913   0.1727   0.1623   0.1835\r\n## Detection Prevalence   0.2851   0.1927   0.1758   0.1625   0.1839\r\n## Balanced Accuracy      0.9991   0.9933   0.9935   0.9949   0.9992</code></pre>\r\n<p>Our Random Forest algorithm generates a model with accuracy 0.994 on our validation data. Since this is satisfactory, there is no need to go back and include more variables with imputations. However, we will still look at which variables are the most important.</p>\r\n<pre class=\"r\"><code>varImp(model.rf, scale = TRUE)</code></pre>\r\n<pre><code>##                        Overall\r\n## pitch_belt           614.51519\r\n## yaw_belt             826.64985\r\n## total_accel_belt     278.27631\r\n## gyros_belt_x          99.36361\r\n## gyros_belt_y         126.55245\r\n## gyros_belt_z         331.28747\r\n## magnet_belt_x        227.95236\r\n## magnet_belt_y        412.54980\r\n## magnet_belt_z        412.10509\r\n## roll_arm             282.20661\r\n## pitch_arm            144.35650\r\n## yaw_arm              194.87720\r\n## total_accel_arm       90.87674\r\n## gyros_arm_x          134.50260\r\n## gyros_arm_z           57.43171\r\n## accel_arm_x          195.56975\r\n## accel_arm_y          145.96397\r\n## accel_arm_z          124.56280\r\n## magnet_arm_x         209.18897\r\n## magnet_arm_y         186.65258\r\n## magnet_arm_z         162.54059\r\n## roll_dumbbell        333.58729\r\n## pitch_dumbbell       153.25792\r\n## yaw_dumbbell         225.44459\r\n## total_accel_dumbbell 222.36165\r\n## gyros_dumbbell_y     221.46724\r\n## gyros_dumbbell_z      81.74818\r\n## accel_dumbbell_x     207.15664\r\n## accel_dumbbell_y     324.12228\r\n## accel_dumbbell_z     283.45309\r\n## magnet_dumbbell_x    391.91952\r\n## magnet_dumbbell_y    549.68404\r\n## magnet_dumbbell_z    623.31366\r\n## roll_forearm         497.15960\r\n## pitch_forearm        607.12650\r\n## yaw_forearm          149.73222\r\n## total_accel_forearm  100.91705\r\n## gyros_forearm_x       74.26957\r\n## gyros_forearm_y      120.54555\r\n## accel_forearm_x      257.38150\r\n## accel_forearm_y      120.79011\r\n## accel_forearm_z      214.52282\r\n## magnet_forearm_x     186.94267\r\n## magnet_forearm_y     190.61587\r\n## magnet_forearm_z     240.44768</code></pre>\r\n<p>The four most important variables according to the model fit are ‘yaw_belt’, ‘pitch_forearm’, ‘magnet_dumbell_z’ and ‘pitch_belt’. These variables have the most effect on the model’s predictive performance.</p>\r\n<p>The model could be fine-tuned to use only the most important predictors. Although this could increase the speed of the model without reducing accuracy significantly, this step is not needed for this project.</p>\r\n</div>\r\n<div id=\"conclusion\" class=\"section level4\">\r\n<h4>Conclusion</h4>\r\n<p>A Random Forest model was created to predict the manner that an exercise was performed given relevant data predicting five classes (‘A’, ‘B’, ‘C’, ‘D’, ‘E’) using 45 predictors. The model We used 53 variables from the training dataset to build a random forest model with four-fold cross validation. The accuracy of the model is 0.988 tested on the out-of-sample data above.</p>\r\n</div>\r\n</div>\r\n</div>\r\n\r\n\r\n</div>\r\n\r\n<script>\r\n\r\n// add bootstrap table styles to pandoc tables\r\n$(document).ready(function () {\r\n  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');\r\n});\r\n\r\n</script>\r\n\r\n<!-- dynamically load mathjax for compatibility with self-contained -->\r\n<script>\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n</script>\r\n\r\n</body>\r\n</html>\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}